---
title: "Collate CPUE data"
author: "Max Lindmark"
date: "`r format(Sys.time(), '%d %B, %Y')`"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align ='center'
)

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

# Intro
In this script, I load exchange data from datras and calculate catch of cod and flounder in unit kg/km^2 (with TVL gear) by size group, by correcting for gear dimensions, sweeplength and trawl speed, following Orio et al 2017. 

## Load libraries

```{r, message=FALSE}
library(tidyverse)
library(readxl)
library(tidylog)
library(RCurl)
library(viridis)
library(RColorBrewer)
library(patchwork)
library(janitor)
library(icesDatras)
library(mapdata)
library(patchwork)
library(rgdal)
library(raster)
library(sf)
library(rgeos)
library(chron)
library(lattice)
library(ncdf4)
library(marmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(mapplots)
library(geosphere)
library(modelr)
library(devtools)
library(terra)

world <- ne_countries(scale = "medium", returnclass = "sf")

# function to plot proportion zero catches
prop_zero <- function(.data){

  .data |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100)

}

# Source code for map plots
source_url("https://raw.githubusercontent.com/maxlindmark/cod-interactions/main/R/functions/map-plot.R")

# Source code for lon lat to utm
source_url("https://raw.githubusercontent.com/maxlindmark/cod-interactions/main/R/functions/lon-lat-utm.R")

theme_set(theme_plot())
```

## Read data

```{r read data, message=FALSE}
# Data were read in from getDATRAS on 2022.09.06
# Read HH data
# bits_hh <- getDATRAS(record = "HH", survey = "BITS", years = 1991:2020, quarters = c(1, 4))
# write.csv(bits_hh, "data/DATRAS_exchange/bits_hh.csv")
bits_hh <- read.csv("data/DATRAS_exchange/bits_hh.csv") |> filter(Year > 1992) # To match covariates

# Read HL data
# bits_hl <- getDATRAS(record = "HL", survey = "BITS", years = 1991:2020, quarters = c(1, 4))
# write.csv(bits_hl, "data/DATRAS_exchange/bits_hl.csv")
bits_hl <- read.csv("data/DATRAS_exchange/bits_hl.csv") |> filter(Year > 1992) # To match covariates

# Read CA data
# bits_ca <- getDATRAS(record = "CA", survey = "BITS", years = 1991:2020, quarters = c(1, 4))
# write.csv(bits_ca, "data/DATRAS_exchange/bits_ca.csv")
bits_ca <- read.csv("data/DATRAS_exchange/bits_ca.csv") |> filter(Year > 1992) # To match covariates

# Read gear standardization data 
sweep <- read.csv("data/from_ale/sweep_9116.csv", sep = ";", dec = ",", fileEncoding = "latin1")
sweep <- read.csv("data/from_ale/sweep_9118_ml.csv", sep = ";", fileEncoding = "latin1")
```

## Standardize catch data
#### Standardize ships
```{r, message=FALSE}
# Before creating a a new ID, make sure that countries and ships names use the same format
sort(unique(sweep$Ship))
sort(unique(bits_hh$Ship))
sort(unique(bits_hl$Ship))

# Change back to the old Ship name standard...
# https://vocab.ices.dk/?ref=315
# https://vocab.ices.dk/?ref=315
# Assumptions:
# SOL is Solea on ICES links above, and SOL1 is the older one of the two SOLs (1 and 2)
# DAN is Dana
# sweep |> filter(Ship == "DANS") |> distinct(Year, Country)
# sweep |> filter(Ship == "DAN2") |> distinct(Year)
# bits_hh |> filter(Ship == "67BC") |> distinct(Year, Country)
# sweep |> filter(Ship == "DAN2") |> distinct(Year)
# bits_hh |> filter(Ship == "26D4") |> distinct(Year) # Strange that 26DF doesn't extend far back. Which ship did the Danes use? Ok, I have no Danish data that old.
# bits_hh |> filter(Country == "DK") |> distinct(Year)

bits_hh <- bits_hh |>
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) |> 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

bits_hl <- bits_hl |>
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) |> 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

bits_ca <- bits_ca |>
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) |> 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

# Ok, which ships are missing in the exchange data?
unique(bits_hh$Ship3)[!unique(bits_hh$Ship3) %in% unique(sweep$Ship)]
# Swedish Ships and unidentified ships are NOT in the Sweep data
unique(sweep$Ship3)[!unique(sweep$Ship3) %in% unique(bits_hh$Ship3)]
# But all Sweep Ships are in the exchange data
```

#### Standardize countries

```{r, message=FALSE}
# Now check which country codes are used
sort(unique(sweep$Country))
sort(unique(bits_hh$Country))

# https://www.nationsonline.org/oneworld/country_code_list.htm#E
bits_hh <- bits_hh |>
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

bits_hl <- bits_hl |>
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

bits_ca <- bits_ca |>
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

# Gear? Are they the same?
sort(unique(bits_hh$Gear))
sort(unique(bits_hl$Gear))
sort(unique(sweep$Gear))

# Which gears are NOT in the sweep data?
unique(bits_hl$Gear)[!unique(bits_hl$Gear) %in% unique(sweep$Gear)] 
```

#### Create a simple haul ID that works across all exchange data

```{r, message=FALSE}
# Create ID column
bits_ca <- bits_ca |> 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_hl <- bits_hl |> 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_hh <- bits_hh |> 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

# Works like a haul-id
bits_hh |> group_by(IDx) |> mutate(n = n()) |> ungroup() |> distinct(n)
```

#### Create the same unique haul-ID in the cpue data that I have in the sweep-file

```{r, message=FALSE}
bits_hl <- bits_hl |> 
  mutate(haul.id = paste(Year, Quarter, Country, Ship3, Gear, StNo, HaulNo, sep = ":")) 

bits_hh <- bits_hh |> 
  mutate(haul.id = paste(Year, Quarter, Country, Ship3, Gear, StNo, HaulNo, sep = ":")) 

bits_hh |> group_by(haul.id) |> mutate(n = n()) |> ungroup() |> distinct(n)
```

#### Clean DATRAS EXCHANGE data

```{r, message=FALSE}
# Select just valid, additional and no oxygen hauls
bits_hh <- bits_hh |>
  #filter(!Country == "SWE") |> # I'll deal with Sweden later...
  filter(HaulVal %in% c("A","N","V"))

# Add ICES rectangle
bits_hh$Rect <- mapplots::ices.rect2(lon = bits_hh$ShootLong, lat = bits_hh$ShootLat)

# Add ICES subdivisions
shape <- shapefile("data/ICES_StatRec_mapto_ICES_Areas/StatRec_map_Areas_Full_20170124.shp")

pts <- SpatialPoints(cbind(bits_hh$ShootLong, bits_hh$ShootLat), 
                     proj4string = CRS(proj4string(shape)))

bits_hh$sub_div <- over(pts, shape)$Area_27

# Rename subdivisions to the more common names and do some more filtering (by sub div and area)
sort(unique(bits_hh$sub_div))

bits_hh <- bits_hh |> 
  mutate(sub_div = factor(sub_div),
         sub_div = fct_recode(sub_div,
                              "20" = "3.a.20",
                              "21" = "3.a.21",
                              "22" = "3.c.22",
                              "23" = "3.b.23",
                              "24" = "3.d.24",
                              "25" = "3.d.25",
                              "26" = "3.d.26",
                              "27" = "3.d.27",
                              "28" = "3.d.28.1",
                              "28" = "3.d.28.2",
                              "29" = "3.d.29"),
         sub_div = as.character(sub_div)) 

# Now add the fishing line information from the sweep file (we need that later
# to standardize based on gear geometry). We add in the the HH data and then
# transfer it to the other exchange data files when left_joining.
# Check which Fishing lines I have in the sweep data:
fishing_line <- sweep |> group_by(Gear) |> distinct(Fishing.line)

bits_hh <- left_join(bits_hh, fishing_line)
# sweep |> group_by(Gear) |> distinct(Fishing.line)
# bits_hh |> group_by(Gear) |> distinct(Fishing.line)
bits_hh$Fishing.line <- as.numeric(bits_hh$Fishing.line)

# Which gears do now have fishing line?
bits_hh$Fishing.line[is.na(bits_hh$Fishing.line)] <- -9
bits_hh |> filter(Fishing.line == -9) |> distinct(Gear)
# 1  GRT
# 2  CAM
# 3  EXP
# 4  FOT
# 5  GOV
# 6  EGY
# 7   DT
# 8  ESB
# 9  HAK

# FROM the index files (Orio, "Research Östersjön 2")
# FOT has 83
# GOV has 160
# ESB ??
# GRT ??
# Rest are unknown and likely not used by Swedish data (therefore their correction
# factors my be in the sweep file)

# Add these values:
bits_hh <- bits_hh |> mutate(Fishing.line = ifelse(Gear == "FOT", 83, Fishing.line))
bits_hh <- bits_hh |> mutate(Fishing.line = ifelse(Gear == "GOV", 160, Fishing.line))

# Now select the hauls in the HH data when subsetting the HL data
bits_hl <- bits_hl |>
  filter(haul.id %in% bits_hh$haul.id)

# Match columns from the HH data to the HL and CA data
sort(unique(bits_hh$sub_div))
sort(colnames(bits_hh))

# No NAs for the variables going in to the stomach haul ID
unique(is.na(bits_hh |> dplyr::select(Year, Quarter, Month, Country, Rect, HaulNo)))

# Before making the id_haul_stomach variable we need to change the country column so that it actually matches the stomach data
# This is the stomach version:
#[1] "LV" "PL" "SE" "DK"
unique(bits_hh$Country)

# MAKE SURE THE COUNTRY CODE IS THE SAME! FOR NOW I DON*T USE COUNTRY 2
bits_hh <- bits_hh |> mutate(Country2 = NA,
                              Country2 = ifelse(Country == "LAT", "LV", Country2),
                              Country2 = ifelse(Country == "POL", "PL", Country2),
                              Country2 = ifelse(Country == "SWE", "SE", Country2),
                              Country2 = ifelse(Country == "DEN", "DK", Country2))

bits_hh_merge <- bits_hh |> 
  mutate(id_haul_stomach = paste(Year, Quarter, Month, Country, Rect, HaulNo, sep = ".")) |> 
  dplyr::select(sub_div, Rect, HaulVal, StdSpecRecCode, BySpecRecCode, Fishing.line, Month,
                DataType, HaulDur, GroundSpeed, haul.id, IDx, ShootLat, ShootLong, id_haul_stomach)

bits_hl <- left_join(dplyr::select(bits_hl, -haul.id), bits_hh_merge, by = "IDx")
bits_ca <- left_join(bits_ca, bits_hh_merge, by = "IDx")

# Now filter the subdivisions I want from all data sets
bits_hh <- bits_hh |> filter(sub_div %in% c(24, 25, 26, 27, 28))
bits_hl <- bits_hl |> filter(sub_div %in% c(24, 25, 26, 27, 28))
bits_ca <- bits_ca |> filter(sub_div %in% c(24, 25, 26, 27, 28))
```

```{r}
bits_hl |> filter(Year == 2016 & Quarter == 1 & Month == 2 & Country == "SWE" & Rect == "39G4") |> distinct(HaulNo)
```

#### Filter species

```{r, message=FALSE}
hlcod <- bits_hl |>
  filter(SpecCode %in% c("126436", "164712")) |> 
  mutate(Species = "Gadus morhua")
```

#### Prepare to add 0 catches

```{r, message=FALSE}
# Find common columns in the HH and HL data (here already subset by species)
comcol <- intersect(names(hlcod), names(bits_hh))

# What is the proportion of zero-catch hauls?
# Here we don't have zero catches
hlcod |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(HLNoAtLngt)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |> 
  distinct(zero_catch)

# Cod: Add 0s and then remove lines with SpecVal = 0 (first NA because we don't have a match in the HH, then make them 0 later)
hlcod0 <- full_join(hlcod, bits_hh[, comcol], by = comcol)

# No zeroes yet
hlcod0 |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(HLNoAtLngt)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |> 
  distinct(zero_catch) 

hlcod0$SpecVal[is.na(hlcod0$SpecVal)] <- "zeroCatch"

hlcod0$SpecVal <- factor(hlcod0$SpecVal)

hlcod0 <-  hlcod0 |> filter(!SpecVal == "0")

# Add species again after merge
hlcod0$Species <- "Gadus morhua"
```

#### Create (unstandardized) CPUE for `SpecVal=1`. If `DataType=C` then `CPUEun=HLNoAtLngt`, if `DataType=R` then `CPUEun=HLNoAtLngt/(HaulDur/60)`, if `DataType=S` then `CPUEun=(HLNoAtLngt*SubFactor)/(HaulDur/60)`. If `SpecVal="zeroCatch"` then `CPUEun=0`, if `SpecVal=4` we need to decide (no length measurements, only total catch). Note that here we also add zero CPUE if `SpecVal=="zeroCatch"`.

Then I will sum for the same haul the CPUE of the same length classes if they were sampled with different subfactors or with different sexes.

```{r, message=FALSE}
# Cod
hlcod0 <- hlcod0 |>
  mutate(CPUEun = ifelse(SpecVal == "1" & DataType == "C",
                         HLNoAtLngt,
                         
                         ifelse(SpecVal == "1" & DataType == "R",
                                HLNoAtLngt/(HaulDur/60),
                                
                                ifelse(SpecVal == "1" & DataType == "S",
                                       (HLNoAtLngt*SubFactor)/(HaulDur/60),
                                       
                                       ifelse(SpecVal == "zeroCatch", 0, NA)))))

# Plot and fill by zero catch
hlcod0 |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  filter(!CPUEun_haul == 0)

hlcod0 |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |>
  group_by(Year, zero_catch) |> 
  summarise(n = n()) |> 
  ggplot(aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")

# Some rows have multiple rows per combination of length class and haul id, so we need to sum it up 
hlcod0 |> group_by(LngtClass, haul.id) |> mutate(n = n()) |> ungroup() |> distinct(n)
hlcod0 |> group_by(LngtClass, haul.id) |> mutate(n = n()) |> ungroup() |> filter(n == 2) |> as.data.frame() |> head(20)
test <- hlcod0 |> group_by(LngtClass, haul.id) |> mutate(n = n()) |> ungroup() |> filter(n == 2)
test_id <- test$haul.id[2]

hlcodL <- hlcod0 |> 
  group_by(LngtClass, haul.id) |> 
  mutate(CPUEun = sum(CPUEun)) |>
  ungroup() |> 
  mutate(id3 = paste(haul.id, LngtClass)) |> 
  distinct(id3, .keep_all = TRUE) |> 
  dplyr::select(-X, -id3) # Clean up a bit

# Check with an ID
filter(hlcod0, haul.id == test_id)
filter(hlcodL, haul.id == test_id) |> as.data.frame()

# Do we still have 0 catches?
hlcodL |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  filter(!CPUEun_haul == 0)

hlcodL |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |>
  group_by(Year, zero_catch) |> 
  summarise(n = n()) |> 
  ggplot(aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")
```

#### Get and add annual weight-length relationships from the CA data for both cod and flounder so that I can calculate CPUE in biomass rather than numbers further down

```{r, message=FALSE}
# Cod
bits_ca_cod <- bits_ca |> 
  filter(SpecCode %in% c("164712", "126436")) |> 
  mutate(StNo = as.numeric(StNo)) |> 
  mutate(Species = "Cod") |> 
  mutate(ID = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

# Now I need to copy rows with NoAtLngt > 1 so that 1 row = 1 ind
# First make a small test
# nrow(bits_ca_cod)
# test_id <- head(filter(bits_ca_cod, CANoAtLngt == 5))$ID[1]
# filter(bits_ca_cod, ID == test_id & CANoAtLngt == 5)

# magritte pipe here to be able to use "."
bits_ca_cod <- bits_ca_cod %>% map_df(., rep, .$CANoAtLngt)

# head(data.frame(filter(bits_ca_cod, ID == test_id & CANoAtLngt == 5)), 20)
# nrow(bits_ca_cod)
# Looks ok!

# Standardize length and drop NA weights (need that for condition)
bits_ca_cod <- bits_ca_cod |> 
  drop_na(IndWgt) |> 
  drop_na(LngtClass) |> 
  filter(IndWgt > 0 & LngtClass > 0) |>  # Filter positive length and weight
  mutate(weight_kg = IndWgt/1000) |> 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) # Standardize length ((https://vocab.ices.dk/?ref=18))

# Plot
ggplot(bits_ca_cod, aes(IndWgt, length_cm)) +
  geom_point() + 
  facet_wrap(~Year)

# Now extract the coefficients for each year (not bothering with outliers at the moment)
cod_intercept <- bits_ca_cod %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "(Intercept)") %>% 
  mutate(a = exp(estimate)) %>% 
  mutate(Year = as.integer(Year)) %>% 
  dplyr::select(Year, a)

cod_slope <- bits_ca_cod %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "log(length_cm)") %>% 
  mutate(Year = as.integer(Year)) %>% 
  rename("b" = "estimate") %>% 
  dplyr::select(Year, b)
```

#### Join the annual L-W relationships to the respective catch data to calculate CPUE in biomass not abundance

```{r, message=FALSE}
# These are the haul-data
# hlcodL

hlcodL <- left_join(hlcodL, cod_intercept, by = "Year")
hlcodL <- left_join(hlcodL, cod_slope, by = "Year")
```

#### Convert from CPUE in numbers to kg

```{r, message=FALSE}
# First standardize length to cm and then check how zero-catches are implemented at this stage
hlcodL <- hlcodL |> 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) # Standardize length ((https://vocab.ices.dk/?ref=18))

filter(hlcodL, length_cm == 0) # No such thing

# Now check if all rows where length is NA are the ones with zero catch!
hlcodL |> 
  mutate(length2 = replace_na(length_cm, -9),
         no_length = ifelse(length2 < 0, "T", "F")) |> 
  ggplot(aes(length2, CPUEun, color = no_length)) + geom_point(alpha = 0.2) +
  facet_wrap(~no_length)

hlcodL |> filter(CPUEun == 0) |> distinct(length_cm)

# Right, so all hauls with zero catch have NA length_cm. I don't have any NA catches
t <- hlcodL |> drop_na(CPUEun)
t <- hlcodL |> filter(CPUEun == 0)
t <- hlcodL |> drop_na(length_cm)

# In other words, a zero catch is when the catch is zero and length_cm is NA
# In order to not get any NA CPUEs in unit biomass because length is NA (I want them instead
# to be 0, as the numbers-CPUE is), I will replace length_cm == NA with length_cm == 0 before
# calculating biomass CPUE
hlcodL <- hlcodL |> mutate(length_cm2 = replace_na(length_cm, 0))

# Standardize length in the haul-data and calculate weight
hlcodL <- hlcodL |> 
  mutate(weight_kg = (a*length_cm2^b)/1000) |> 
  mutate(CPUEun_kg = weight_kg*CPUEun)

# Plot and check it's correct also in this data
ggplot(hlcodL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)

# Hmm, some unrealistic weights actually
hlcodL |> arrange(desc(weight_kg)) |> as.data.frame() |> head(20)
hlcodL <- hlcodL |> filter(weight_kg < 100 & length_cm2 < 135)

ggplot(hlcodL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)

# What is the proportion of zero-catch hauls?
hlcodL |>
  group_by(haul.id) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  filter(CPUEun_haul == 0)
  
cod_0plot <- hlcodL |>
  group_by(haul.id, Year, Quarter) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |> 
  group_by(Year, Quarter, zero_catch) |> 
  summarise(n = n()) |> 
  ungroup() |> 
  pivot_wider(names_from = zero_catch, values_from = n) |> 
  mutate(prop_zero_catch_hauls = Y/(N+Y)) |> 
  ggplot(aes(Year, prop_zero_catch_hauls)) + geom_bar(stat = "identity") + 
  coord_cartesian(expand = 0, ylim = c(0, 1)) + 
  facet_wrap(~ Quarter) +
  ggtitle("Cod")
```

#### Standardize according to Orio
To get unit: kg of fish caught by trawling for 1 h a standard bottom swept area of 0.45km2 using a TVL trawl with 75 m sweeps at the standard speed of three knots

```{r, message=FALSE}
# Remove hauls done with the TVL gear with a SweepLngt < 50 (these are calibration hauls, pers. com. Anders & Ale)
# And also hauls without length-information
# Remove pelagic gear
hlcodL <- hlcodL |>
  mutate(SweepLngt2 = replace_na(SweepLngt, 50)) |> 
  mutate(keep = ifelse(Gear == "TVL" & SweepLngt2 < 50, "N", "Y")) |> 
  filter(keep == "Y") |> 
  dplyr::select(-keep, -SweepLngt2) |> 
  filter(!Gear == "PEL")
  
# Add in RS and RSA-values from the sweep file
# CPUE should be multiplied with RS and RSA to standardize to a relative speed and gear dimension.
# There is not a single file will all RS and RSA values. Instead they come in three files:
# - sweep (non-Swedish hauls between 1991-2016)
# - + calculated based on trawl speed and gear dimensions.
# I will join in the RS and RSA values from all sources, then standardize and filter
# away non-standardized hauls
# sort(unique(sweep$Year))
# sort(unique(sweep$Country))

# Since I don't have the sweep data for Swedish data, I have to calculate it from scratch using the 
# equation in Orio's spreadsheet

# First I will join in the sweep data, 
sweep_sel <- sweep |> rename("haul.id" = "ï..haul.id") |> dplyr::select(haul.id, RSA, RS)

hlcodL2 <- left_join(hlcodL, sweep_sel)

hlcodL2 <- hlcodL2 |>
  rename("RS_sweep" = "RS",
         "RSA_sweep" = "RSA") |> 
  mutate(RS_sweep = as.numeric(RS_sweep),
         RSA_sweep = as.numeric(RSA_sweep))

sort(colnames(hlcodL2))

# I will calculate a RS and RSA column in the catch data based on Ale's equation in the sweep file:
sort(unique(hlcodL2$GroundSpeed))
sort(unique(hlcodL2$Fishing.line))
sort(unique(hlcodL2$SweepLngt))

# First replace -9 in the columns I use for the calculations with NA so I don't end up with real numbers that are wrong!
hlcodL2 <- hlcodL2 |> mutate(GroundSpeed = ifelse(GroundSpeed == -9, NA, GroundSpeed),
                              Fishing.line = ifelse(Fishing.line == -9, NA, Fishing.line),
                              SweepLngt = ifelse(SweepLngt == -9, NA, SweepLngt))

hlcodL2 |> filter(Quarter == 1) |>
  distinct(GroundSpeed, Fishing.line, SweepLngt) |> as.data.frame() |> head(20)

hlcodL2 |> filter(Quarter == 4) |>
  distinct(GroundSpeed, Fishing.line, SweepLngt) |> as.data.frame() |> head(20)

# Hmm, Q1 has at least one of the RS or RSA variables as NAs. Will be difficult to standardize!
# Hope the correction factors are present in Ales conversion data

# Now calculate correction factors
hlcodL2 <- hlcodL2 |> mutate(RS_x = 3/GroundSpeed,
                              Horizontal.opening..m. = Fishing.line*0.67,
                              Swep.one.side..after.formula...meter = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                              Size.final..m = Horizontal.opening..m. + (Swep.one.side..after.formula...meter*2),
                              Swept.area = (Size.final..m*3*1860)/1000000,
                              RSA_x = 0.45388309675081/Swept.area)

# Check EQ. is correct by recalculating it in the sweep file
sweep <- sweep |> mutate(Horizontal.opening..m.2 = Fishing.line*0.67,
                          Swep.one.side..after.formula...meter2 = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                          Size.final..m2 = Horizontal.opening..m.2 + (Swep.one.side..after.formula...meter2*2),
                          Swept.area2 = (Size.final..m2*3*1860)/1000000,
                          RSA_x = 0.45388309675081/Swept.area2)

sweep %>%
  drop_na() %>%
  ggplot(., aes(as.numeric(RSA), RSA_x)) + geom_point() + geom_abline(intercept = 0, slope = 1)
# Yes it's the same

# Replace NAs with -1/3 (because ICES codes missing values as -9 and in the calculation above they get -1/3),
# so that I can filter them easily later
# sort(unique(hlcodL2$RS_x))
# sort(unique(hlcodL2$RSA_x))

hlcodL2$RS_x[is.na(hlcodL2$RS_x)] <- -1/3
hlcodL2$RS_sweep[is.na(hlcodL2$RS_sweep)] <- -1/3
hlcodL2$RSA_x[is.na(hlcodL2$RSA_x)] <- -1/3
hlcodL2$RSA_sweep[is.na(hlcodL2$RSA_sweep)] <- -1/3

# Compare the difference correction factors (calculated vs imported from sweep file)
p1 <- ggplot(filter(hlcodL2, RS_x > 0), aes(RS_x)) + geom_histogram() + xlim(0.4, 1.76)
p2 <- ggplot(hlcodL2, aes(RSA_x)) + geom_histogram()
p3 <- ggplot(hlcodL2, aes(RS_sweep)) + geom_histogram()
p4 <- ggplot(hlcodL2, aes(RSA_sweep)) + geom_histogram()

(p1 + p2) / (p3 + p4)

# Why do I have RSA values smaller than one? (either because sweep length is longer or gear is larger (GOV))
# Check if I can calculate the same RSA in sweep as that entered there.
# Ok, so the equation is correct. Which ID's have RSA < 1?
hlcodL2 |> 
  filter(RSA_x < 1 & RSA_x > 0) |>
  dplyr::select(Year, Country, Ship, Gear, haul.id, Horizontal.opening..m., Fishing.line,
                Swep.one.side..after.formula...meter, SweepLngt, Size.final..m, Swept.area, RSA_x) |> 
  ggplot(aes(RSA_x, fill = factor(SweepLngt))) + geom_histogram() + facet_wrap(~Gear, ncol = 1)

# Check if I have more than one unique RS or RSA value per haul, or if it's "either this or that"
# Filter positive in both columns
hlcodL2 |> filter(RS_x > 0 & RS_sweep > 0) |> 
  ggplot(aes(RS_x, RS_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

hlcodL2 |> filter(RSA_x > 0 & RSA_sweep > 0) |>
  ggplot(aes(RSA_x, RSA_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

# Ok, there's on odd RS_x that is larger than 3. It didn't catch anything and speed is 0.8! Will remove
hlcodL2 <- hlcodL2 |> filter(RS_x < 3)

# Plot again
hlcodL2 |> filter(RS_x > 0 & RS_sweep > 0) |>
  ggplot(aes(RS_x, RS_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

# They are largely the same when they overlap. When they differ, I will use RS_sweep
# Make a single RS and RSA column

# Cod 
hlcodL3 <- hlcodL2 |>
  mutate(RS = -99,
         RS = ifelse(RS_sweep > 0, RS_sweep, RS),
         RS = ifelse(RS < 0 & RS_x > 0, RS_x, RS)) |> # Note that there are no NA i RS_x. This replaces all non-RS_sweep values -0.3, so I can filter positive later
  mutate(RSA = -99,
         RSA = ifelse(RSA_sweep > 0, RSA_sweep, RSA),
         RSA = ifelse(RSA < 0 & RSA_x > 0, RSA_x, RSA)) |>
  filter(RS > 0) |>
  filter(RSA > 0) |> 
  mutate(RSRSA = RS*RSA)

# Plot
ggplot(hlcodL3, aes(RSRSA)) + geom_histogram()
# Plot
ggplot(hlcodL3, aes(RSRSA)) + geom_histogram()

# Standardize!
hlcodL3 <- hlcodL3 |>
  mutate(CPUEst_kg = CPUEun_kg*RS*RSA,
         CPUEst = CPUEun*RS*RSA)

unique(is.na(hlcodL3$CPUEst_kg))
unique(is.na(hlcodL3$CPUEst))
min(hlcodL3$CPUEst_kg)
min(hlcodL3$CPUEst)

# Now calculate CPUE PER LENGTH CLASS, then create the new unit, i.e.:  convert from kg of fish caught by trawling for 1 h a standard bottom swept area of 0.45km2 (using a TVL trawl with 75 m sweeps at the standard speed of three knots) to... kg of fish per km^2 by dividing with 0.45

p1 <- ggplot(hlcodL3) +
  geom_histogram(aes(length_cm2, fill = "length_cm1"), alpha = 0.5)  

p2 <- ggplot(hlcodL3) +
  geom_histogram(aes(length_cm2, fill = "length_cm2"), alpha = 0.5) 

p1/p2

hlcodhaul <- hlcodL3 |>
  mutate(cpue_kg = CPUEst_kg,
         cpue = CPUEst,
         cpue_kg_un = CPUEun_kg,
         cpue_un = CPUEun,
         density = cpue_kg/0.45,
         density_ab = cpue/0.45)

# t <- hlcodhaul |> filter(haul_cpue_un == 0)
# t <- hlcodhaul |> filter(!Country == "SWE") |> filter(haul_cpue_un > 0)

# First, figure out why i have length = 0 and density = 0 when I have other lengths in the haul
hlcodhaul |> filter(haul.id == "1993:1:GFR:SOL:H20:23:31") |> as.data.frame()

hlcodhaul |>
  group_by(haul.id) |> 
  mutate(no_catches = length(unique(CPUEun))) |> 
  filter(any(CPUEun == 0)) |> 
  filter(no_catches > 1) |> 
  as.data.frame() |> 
  head(20)

hlcodhaul |> 
  group_by(haul.id) |> 
  filter(CPUEun == min(CPUEun)) |> 
  ungroup() |> 
  distinct(CPUEun)

# The minimum CPUE in all hauls is always zero at this stage. It doesn't really matter because I calculate haul-level CPUE by grouping by ID's and summing. But let's remove them anyway

hlcodhaul |>
  group_by(haul.id) |>
  summarise(cpue_haul = sum(cpue)) |> 
  ungroup() |> 
  filter(!cpue_haul == 0)

# Rename columns and select specific columns from the cod data
datcod <- hlcodhaul |>
  dplyr::select(density, Year, ShootLat, ShootLong, Quarter, Country, Month, haul.id, IDx, Rect, sub_div, length_cm2, id_haul_stomach) |> 
  rename(year = Year,
         month = Month,
         lat = ShootLat,
         lon = ShootLong,
         quarter = Quarter,
         ices_rect = Rect,
         length_cm = length_cm2) |> 
  mutate(species = "cod")
```

```{r very long data with all combinations of size and haul id}
# Because it's size-based cpue, I want the data frame to be "full", so that each haul has every size, even if all are empty. Now I only have lengths with catches, and no lengths if catch is zero.
datcod |> group_by(haul.id) |> summarise(n_size = length(unique(length_cm))) |> distinct(n_size, .keep_all = TRUE)
datcod |> filter(haul.id == "1993:1:GFR:SOL:H20:23:31") |> as.data.frame()
datcod |> group_by(haul.id) |> mutate(tot_dens = sum(density)) |> ungroup() |> distinct(haul.id, .keep_all = TRUE) |> filter(tot_dens == 0)

# Create a data frame with all combinations of trawl IDs and lengths
ex_df <- data.frame(expand.grid(
  length_cm = seq_range(datcod$length_cm, by = 1),
  haul.id = unique(datcod$haul.id))
  )

# Create an ID that is haul + length
ex_df$haul.id.size <- paste(ex_df$haul.id, ex_df$length_cm, sep = ".")
datcod$haul.id.size <- paste(datcod$haul.id, datcod$length_cm, sep = ".")

# Remove IDs that are already in datcod
ex_df <- ex_df |> filter(!haul.id.size %in% unique(datcod$haul.id.size)) 

# Add in the other columns besides density and length
dat_for_join <- datcod |> dplyr::select(-density, -length_cm, -haul.id.size) |> distinct(haul.id, .keep_all = TRUE)

ex_df <- left_join(ex_df, dat_for_join, by = "haul.id")

datcod |> filter(haul.id.size %in% ex_df$haul.id.size)

# Bind_rows these data with datcod
nrow(datcod) + nrow(ex_df)

unique(is.na(datcod$density))

datcod <- bind_rows(datcod, ex_df) |> arrange(haul.id, length_cm)
nrow(datcod)
datcod

# Replace NA density with 0 density because that's the added length-classes not previously in the catch data
datcod <- datcod |> mutate(density = replace_na(density, 0))

# Check the proportion zeroes are still correct:
t <- datcod |>
  group_by(haul.id) |>
  summarise(haul_density = sum(density)) |> 
  ungroup()

nrow(datcod)
length(unique(datcod$haul.id))
nrow(t)
t |> filter(!haul_density == 0)

# Rename
dat <- datcod

glimpse(dat)

# Check proportion zeroes
dat |> 
  group_by(haul.id) |> 
  summarise(haul_dens = sum(density)) |> 
  ungroup() |> 
  filter(!haul_dens == 0)

codq4 <- dat |> 
  filter(species == "cod" & quarter == 4) |> 
  group_by(haul.id) |> 
  mutate(haul_dens = sum(density)) |> 
  distinct(haul.id, .keep_all = TRUE) |> 
  mutate(zero_catch = ifelse(haul_dens == 0, "Y", "N")) |> 
  group_by(year, zero_catch) |> 
  summarise(n = n()) |> 
  ungroup() |> 
  pivot_wider(names_from = zero_catch, values_from = n) |> 
  mutate(prop_z = Y / (N+Y), 
         q = 4,
         species = "cod")  

codq1 <- dat |> 
  filter(species == "cod" & quarter == 1) |> 
  group_by(haul.id) |> 
  mutate(haul_dens = sum(density)) |> 
  distinct(haul.id, .keep_all = TRUE) |> 
  mutate(zero_catch = ifelse(haul_dens == 0, "Y", "N")) |> 
  group_by(year, zero_catch) |> 
  summarise(n = n()) |> 
  ungroup() |> 
  pivot_wider(names_from = zero_catch, values_from = n) |> 
  mutate(prop_z = Y / (N+Y), 
         q = 1,
         species = "cod")  

ggplot(bind_rows(codq1, codq4), aes(year, prop_z*100, color = factor(q))) +
  geom_line() +
  facet_wrap(~ species, ncol = 1)
```

## Add in the environmental variables

```{r}
# Only need 1 row per haul
dat <- dat %>%
  mutate(month_year = paste(month, year, sep = "_"))
  
dat_haul <- dat %>%
  distinct(haul.id, .keep_all = TRUE) |>
  dplyr::select(lat, lon, year, month, quarter, month_year)
```

#### Oxygen

```{r}
# Downloaded from here: https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=BALTICSEA_REANALYSIS_BIO_003_012
# Extract raster points: https://gisday.wordpress.com/2014/03/24/extract-raster-values-from-points-using-r/comment-page-1/
# https://rpubs.com/boyerag/297592
# https://pjbartlein.github.io/REarthSysSci/netCDF.html#get-a-variable
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-scobi-monthlymeans_1664182224542.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get oxygen
dname <- "o2b"

oxy_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(oxy_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
oxy_array[oxy_array == fillvalue$value] <- NA

dim(oxy_array)
str(dim(oxy_array))
# The third slot is the date index

# Loop through all "dates", put into a list 
dlist <- list()

for(i in 1:length(months)) {
  
  oxy_sub <- oxy_array[, , i]
    
  dlist[[i]] <- oxy_sub
  
}

# Name the list
names(dlist) <- paste(months, years, sep = "_")
str(dlist)

# Create data holding object
oxy_data_list <- list()

# Loop through each month_year and extract raster values for the cpue data points
for(i in unique(dat_haul$month_year)) { # We can use q1 as looping index, doesn't matter!
  
  # Set plot limits
  ymin = 54; ymax = 58; xmin = 12; xmax = 22

  # Subset a month-year combination
  oxy_slice <- dlist[[i]]
  
  # Create raster for that year (i)
  r <- raster(t(oxy_slice), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
              crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r <- flip(r, direction = 'y')
  
  plot(r, main = paste(i), ylim = c(ymin, ymax), xlim = c(xmin, xmax))

  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice <- dat_haul %>% filter(month_year == i) %>% dplyr::select(lon, lat)
  
  # Make into a SpatialPoints object
  data_sp <- SpatialPoints(d_slice)
  
  # Extract raster value (oxygen)
  rasValue <- raster::extract(r, data_sp)
  
  # Now we want to plot the results of the raster extractions by plotting the cpue data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for plot)
  df <- as.data.frame(data_sp)
  
  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice$oxy <- rasValue
  
  # Add in which year
  d_slice$month_year <- i

  # Now the unit of oxygen is mmol/m3. I want it to be ml/L. The original model is in unit ml/L
  # and it's been converted by the data host. Since it was converted without accounting for
  # pressure or temperature, I can simply use the following conversion factor:
  # 1 ml/l = 103/22.391 = 44.661 μmol/l -> 1 ml/l = 0.044661 mmol/l = 44.661 mmol/m^3 -> 0.0223909 ml/l = 1mmol/m^3
  # https://ocean.ices.dk/tools/unitconversion.aspx

  d_slice$oxy <- d_slice$oxy * 0.0223909
    
  # Add each years' data in the list
  oxy_data_list[[i]] <- d_slice

}

# Now create a data frame from the list of all annual values
big_dat_oxy <- dplyr::bind_rows(oxy_data_list)
```

#### Temperature

```{r}
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-nemo-monthlymeans_1664183191233.nc")
                                        
print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get temperature
dname <- "bottomT"

temp_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(temp_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
temp_array[temp_array == fillvalue$value] <- NA

# Loop through all "dates", put into a list 
dlist <- list()

for(i in 1:length(months)) {
  
  temp_sub <- temp_array[, , i]
  
  dlist[[i]] <- temp_sub
  
}

# Name the list
names(dlist) <- paste(months, years, sep = "_")
str(dlist)

# Create data holding object
temp_data_list <- list()

# Loop through each month_year and extract raster values for the cpue data points
for(i in unique(dat_haul$month_year)) { # We can use q1 as looping index, doesn't matter!
  
  # Set plot limits
  ymin = 54; ymax = 58; xmin = 12; xmax = 22
  
  # Subset a month-year combination
  temp_slice <- dlist[[i]]
  
  # Create raster for that year (i)
  r <- raster(t(temp_slice), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
              crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r <- flip(r, direction = 'y')
  
  plot(r, main = paste(i), ylim = c(ymin, ymax), xlim = c(xmin, xmax))
  
  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice <- dat_haul %>% filter(month_year == i) %>% dplyr::select(lon, lat)
  
  # Make into a SpatialPoints object
  data_sp <- SpatialPoints(d_slice)
  
  # Extract raster value (oxygen)
  rasValue <- raster::extract(r, data_sp)
  
  # Now we want to plot the results of the raster extractions by plotting the cpue data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for plot)
  df <- as.data.frame(data_sp)
  
  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice$temp <- rasValue
  
  # Add in which year
  d_slice$month_year <- i
  
  # Add each years' data in the list
  temp_data_list[[i]] <- d_slice
  
}

# Now create a data frame from the list of all annual values
big_dat_temp <- dplyr::bind_rows(temp_data_list)
```

#### Bottom salinity

```{r}
# https://data.marine.copernicus.eu/product/BALTICSEA_REANALYSIS_PHY_003_011/download?dataset=dataset-reanalysis-nemo-monthlymeans

# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-nemo-monthlymeans_1668587452211.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get Salinity
dname <- "sob"

sal_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(sal_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
sal_array[sal_array == fillvalue$value] <- NA

# Loop through all "dates", put into a list 
dlist <- list()

for(i in 1:length(months)) {
  
  sal_sub <- sal_array[, , i]
  
  dlist[[i]] <- sal_sub
  
}

# Name the list
names(dlist) <- paste(months, years, sep = "_")
str(dlist)

# Create data holding object
sal_data_list <- list()

# Loop through each month_year and extract raster values for the cpue data points
for(i in unique(dat_haul$month_year)) { # We can use q1 as looping index, doesn't matter!
  
  # Set plot limits
  ymin = 54; ymax = 58; xmin = 12; xmax = 22
  
  # Subset a month-year combination
  sal_slice <- dlist[[i]]
  
  # Create raster for that year (i)
  r <- raster(t(sal_slice), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
              crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r <- flip(r, direction = 'y')
  
  plot(r, main = paste(i), ylim = c(ymin, ymax), xlim = c(xmin, xmax))
  
  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice <- dat_haul %>% filter(month_year == i) %>% dplyr::select(lon, lat)
  
  # Make into a SpatialPoints object
  data_sp <- SpatialPoints(d_slice)
  
  # Extract raster value (oxygen)
  rasValue <- raster::extract(r, data_sp)
  
  # Now we want to plot the results of the raster extractions by plotting the cpue data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for plot)
  df <- as.data.frame(data_sp)
  
  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice$sal <- rasValue
  
  # Add in which year
  d_slice$month_year <- i
  
  # Add each years' data in the list
  sal_data_list[[i]] <- d_slice
  
}

# Now create a data frame from the list of all annual values
big_dat_sal <- dplyr::bind_rows(sal_data_list)
```

```{r merge oxygen temp and salinity data with fish data}
env_dat <- left_join(big_dat_oxy, big_dat_temp, by = c("month_year", "lon", "lat")) |>
  left_join(big_dat_sal, by = c("month_year", "lon", "lat"))
```

#### Depth

```{r}
# Only use unique locations and then left_join else it will take forever
# https://gis.stackexchange.com/questions/411261/read-multiple-layers-raster-from-ncdf-file-using-terra-package
# https://emodnet.ec.europa.eu/geoviewer/
dep_raster <- terra::rast("data/Mean depth natural colour (with land).nc")

class(dep_raster)
plot(dep_raster)

env_dat$depth <- terra::extract(dep_raster, env_dat |> dplyr::select(lon, lat))$elevation

ggplot(env_dat, aes(lon, lat, color = depth*-1)) + 
  geom_point() + 
  scale_color_viridis(direction = -1)

env_dat$depth <- env_dat$depth*-1
```

#### Substrate

```{r}
substrate <- terra::rast("data/substrate_tif/BALANCE_SEABED_SEDIMENT.tif")

newcrs <- "+proj=longlat +datum=WGS84"

substrate_longlat = terra::project(substrate, newcrs)
 
plot(substrate_longlat)

# Now extract the values from the saduria raster to dat
env_dat$substrate <- terra::extract(substrate_longlat, env_dat |> dplyr::select(lon, lat))$BALANCE_SEABED_SEDIMENT

unique(env_dat$substrate)

factor(sort(unique(round(env_dat$substrate))))
 
env_dat$substrate <- round(env_dat$substrate)

env_dat <- env_dat %>% mutate(substrate = ifelse(substrate == 1, "bedrock", substrate),
                              substrate = ifelse(substrate == 2, "hard-bottom complex",
                                                 substrate),
                              substrate = ifelse(substrate == 3, "sand", substrate),
                              substrate = ifelse(substrate == 4, "hard clay", substrate),
                              substrate = ifelse(substrate == 5, "mud", substrate))

# I. Bedrock.
# II. Hard bottom complex, includes patchy hard surfaces and coarse sand (sometimes also clay) to boulders.
# III. Sand including fine to coarse sand (with gravel exposures).
# IV. Hard clay sometimes/often/possibly exposed or covered with a thin layer of sand/gravel.
# V. Mud including gyttja-clay to gyttja-silt.

# Plot
ggplot(env_dat, aes(lon, lat, color = substrate)) +
  geom_point()
```

```{r}
# Now join these data with the full_dat
dat_full <- left_join(dat, env_dat, by = c("month_year", "lon", "lat"))
```

## Add UTM coords

```{r}
# Add UTM coords
utm_coords <- LongLatToUTM(dat_full$lon, dat_full$lat, zone = 33)
dat_full$X <- utm_coords$X/1000 # for computational reasons we do it in km
dat_full$Y <- utm_coords$Y/1000
```

## Save data

```{r}
dat_full_save <- dat_full |>
  dplyr::select(-IDx, -haul.id.size) |> 
  janitor::clean_names()

write.csv(dat_full_save, file = "data/clean/catch_by_length.csv", row.names = FALSE)
```

#### Compare cod data with Orio et al (2017)

```{r}
knitr::knit_exit()
```

Check this later...

```{r, message=FALSE}
# Read Orio data first for comparison (testing total haul-level CPUE here, not size-specific)
test_cod <- hlcodhaul |> filter(!Country == "SWE")
test_fle <- hlflehaul |> filter(!Country == "SWE")

orio_cod <- read.csv("/Users/maxlindmark/Dropbox/Max work/R/gear_standardization_ale/datras_st_ale.csv")
orio_fle <- read.csv("/Users/maxlindmark/Dropbox/Max work/R/gear_standardization_ale/datras_fle_st_ale.csv")

orio_cod <- orio_cod |>
  group_by(IDX) |>
  mutate(haul_cpue_kg = sum(CPUEstBIOyear),
         haul_cpue = sum(CPUEst),
         haul_cpue_kg_un = sum(CPUEunBIOyear),
         haul_cpue_un = sum(CPUEun)) |>
  ungroup() |>
  distinct(IDX, .keep_all = TRUE)

orio_fle <- orio_fle |>
  group_by(IDX) |>
  mutate(haul_cpue_kg = sum(CPUEstBIOyear),
         haul_cpue = sum(CPUEst),
         haul_cpue_kg_un = sum(CPUEunBIOyear),
         haul_cpue_un = sum(CPUEun)) |>
  ungroup() |>
  distinct(IDX, .keep_all = TRUE)

# hlcodhaul |> group_by(IDx) |> mutate(n = n()) |> ungroup() |> distinct(n)

# # Standardize data for easier plotting
# test_cod |>
#   group_by(haul.id) |>
#   summarise(n = n()) |>
#   distinct(n)

test_cod_q1 <- test_cod |>
  group_by(haul.id) |>
  mutate(haul_cpue_kg = sum(haul_cpue_kg),
         haul_cpue = sum(haul_cpue),
         haul_cpue_kg_un = sum(haul_cpue_kg_un),
         haul_cpue_un = sum(haul_cpue_un),
         density = haul_cpue_kg/0.45,
         density_ab = haul_cpue/0.45) |>
  distinct(haul.id, .keep_all = TRUE) |> 
  dplyr::select(haul_cpue_kg, haul_cpue, haul_cpue_kg_un, haul_cpue_un, Year, Ship3, Country, Gear, Quarter) |> 
  mutate(source = "Max",
         species = "Cod") |> 
  rename("Ship" = "Ship3") |> 
  filter(Year > 1992 & Year < 2017 & Quarter == 1)

test_cod_q4 <- test_cod |>
  group_by(haul.id) |>
  mutate(haul_cpue_kg = sum(haul_cpue_kg),
         haul_cpue = sum(haul_cpue),
         haul_cpue_kg_un = sum(haul_cpue_kg_un),
         haul_cpue_un = sum(haul_cpue_un),
         density = haul_cpue_kg/0.45,
         density_ab = haul_cpue/0.45) |>
  distinct(haul.id, .keep_all = TRUE) |> 
  dplyr::select(haul_cpue_kg, haul_cpue, haul_cpue_kg_un, haul_cpue_un, Year, Ship3, Country, Gear, Quarter) |>
  mutate(source = "Max",
         species = "Cod") |> 
  rename("Ship" = "Ship3") |> 
  filter(Year > 1992 & Year < 2017 & Quarter == 4)

test_fle_q1 <- test_fle |>
  group_by(haul.id) |>
  mutate(haul_cpue_kg = sum(haul_cpue_kg),
         haul_cpue = sum(haul_cpue),
         haul_cpue_kg_un = sum(haul_cpue_kg_un),
         haul_cpue_un = sum(haul_cpue_un),
         density = haul_cpue_kg/0.45,
         density_ab = haul_cpue/0.45) |> 
  distinct(haul.id, .keep_all = TRUE) |> 
  dplyr::select(haul_cpue_kg, haul_cpue, haul_cpue_kg_un, haul_cpue_un, Year, Ship3, Country, Gear, Quarter) |> 
  mutate(source = "Max",
         species = "Fle") |> 
  rename("Ship" = "Ship3") |> 
  filter(Year > 1992 & Year < 2017 & Quarter == 1)

test_fle_q4 <- test_fle |>
    group_by(haul.id) |>
    mutate(haul_cpue_kg = sum(haul_cpue_kg),
         haul_cpue = sum(haul_cpue),
         haul_cpue_kg_un = sum(haul_cpue_kg_un),
         haul_cpue_un = sum(haul_cpue_un),
         density = haul_cpue_kg/0.45,
         density_ab = haul_cpue/0.45) |> 
  distinct(haul.id, .keep_all = TRUE) |> 
  dplyr::select(haul_cpue_kg, haul_cpue, haul_cpue_kg_un, haul_cpue_un, Year, Ship3, Country, Gear, Quarter) |> 
  mutate(source = "Max",
         species = "Fle") |> 
  rename("Ship" = "Ship3") |> 
  filter(Year > 1992 & Year < 2017 & Quarter == 4)

orio_cod_q1 <- orio_cod |>
  dplyr::select(haul_cpue_kg, haul_cpue, haul_cpue_kg_un, haul_cpue_un, year, vessel, IDX) |> 
  mutate(source = "Ale",
         species = "Cod") |> 
  rename("Year" = "year",
         "Ship" = "vessel") |> 
  mutate(IDX2 = IDX,
         IDX3 = IDX) |> 
  separate(IDX, sep = c(5:7), into = c("temp_year", "Quarter")) |> 
  separate(temp_year, sep = c(4:5), into = c("Year", "scrap")) |> 
  separate(IDX2, sep = 7, into = c("scrap2", "Country")) |>
  separate(Country, sep = 3, into = c("Country", "scrap3")) |>
  separate(IDX3, sep = 15, into = c("scrap4", "Gear")) |>
  separate(Gear, sep = 3, into = c("Gear", "scrap5")) |> 
  dplyr::select(-scrap, -scrap2, -scrap3, -scrap4, -scrap5) |> 
  filter(Quarter == 1) |> 
  mutate(Year = as.integer(Year),
         Quarter = as.integer(Quarter)) |> 
  mutate(haul_cpue_kg = haul_cpue_kg/1000,
         haul_cpue_kg_un = haul_cpue_kg_un/1000) |> 
  filter(Year > 1992) |> 
  filter(Country %in% unique(test_cod$Country))

orio_cod_q4 <- orio_cod |>
  dplyr::select(haul_cpue_kg, haul_cpue, haul_cpue_kg_un, haul_cpue_un, year, vessel, IDX) |> 
  mutate(source = "Ale",
         species = "Cod") |> 
  rename("Year" = "year",
         "Ship" = "vessel") |> 
  mutate(IDX2 = IDX,
         IDX3 = IDX) |> 
  separate(IDX, sep = c(5:7), into = c("temp_year", "Quarter")) |> 
  separate(temp_year, sep = c(4:5), into = c("Year", "scrap")) |> 
  separate(IDX2, sep = 7, into = c("scrap2", "Country")) |>
  separate(Country, sep = 3, into = c("Country", "scrap3")) |>
  separate(IDX3, sep = 15, into = c("scrap4", "Gear")) |>
  separate(Gear, sep = 3, into = c("Gear", "scrap5")) |> 
  dplyr::select(-scrap, -scrap2, -scrap3, -scrap4, -scrap5) |> 
  filter(Quarter == 4) |> 
  mutate(Year = as.integer(Year),
         Quarter = as.integer(Quarter)) |> 
  mutate(haul_cpue_kg = haul_cpue_kg/1000,
         haul_cpue_kg_un = haul_cpue_kg_un/1000) |> 
  filter(Year > 1992) |> 
  filter(Country %in% unique(test_cod$Country))

orio_fle_q1 <- orio_fle |> 
  dplyr::select(haul_cpue_kg, haul_cpue, haul_cpue_kg_un, haul_cpue_un, year, vessel, IDX) |> 
  mutate(source = "Ale",
         species = "Fle") |> 
  rename("Year" = "year",
         "Ship" = "vessel") |> 
  mutate(IDX2 = IDX,
         IDX3 = IDX) |> 
  separate(IDX, sep = c(5:7), into = c("temp_year", "Quarter")) |> 
  separate(temp_year, sep = c(4:5), into = c("Year", "scrap")) |> 
  separate(IDX2, sep = 7, into = c("scrap2", "Country")) |>
  separate(Country, sep = 3, into = c("Country", "scrap3")) |>
  separate(IDX3, sep = 15, into = c("scrap4", "Gear")) |>
  separate(Gear, sep = 3, into = c("Gear", "scrap5")) |> 
  dplyr::select(-scrap, -scrap2, -scrap3, -scrap4, -scrap5) |> 
  filter(Quarter == 1) |> 
  mutate(Year = as.integer(Year),
         Quarter = as.integer(Quarter)) |> 
  mutate(haul_cpue_kg = haul_cpue_kg/1000,
         haul_cpue_kg_un = haul_cpue_kg_un/1000) |> 
  filter(Year > 1992) |> 
  filter(Country %in% unique(test_cod$Country))

orio_fle_q4 <- orio_fle |> 
  dplyr::select(haul_cpue_kg, haul_cpue, haul_cpue_kg_un, haul_cpue_un, year, vessel, IDX) |> 
  mutate(source = "Ale",
         species = "Fle") |> 
  rename("Year" = "year",
         "Ship" = "vessel") |> 
  mutate(IDX2 = IDX,
         IDX3 = IDX) |> 
  separate(IDX, sep = c(5:7), into = c("temp_year", "Quarter")) |> 
  separate(temp_year, sep = c(4:5), into = c("Year", "scrap")) |> 
  separate(IDX2, sep = 7, into = c("scrap2", "Country")) |>
  separate(Country, sep = 3, into = c("Country", "scrap3")) |>
  separate(IDX3, sep = 15, into = c("scrap4", "Gear")) |>
  separate(Gear, sep = 3, into = c("Gear", "scrap5")) |> 
  dplyr::select(-scrap, -scrap2, -scrap3, -scrap4, -scrap5) |> 
  filter(Quarter == 4) |> 
  mutate(Year = as.integer(Year),
         Quarter = as.integer(Quarter)) |> 
  mutate(haul_cpue_kg = haul_cpue_kg/1000,
         haul_cpue_kg_un = haul_cpue_kg_un/1000) |> 
  filter(Year > 1992) |> 
  filter(Country %in% unique(test_cod$Country))

sort(unique(test_cod_q1$Country))
sort(unique(orio_cod_q1$Country))
```

```{r proportion 0 comparison}
prop_zero(test_cod_q1)
prop_zero(orio_cod_q1)

prop_zero(test_cod_q4)
prop_zero(orio_cod_q4)

prop_zero(test_fle_q1)
prop_zero(orio_fle_q1)

prop_zero(test_fle_q4)
prop_zero(orio_fle_q4)

# Plot proportion of 0 catches by year and country
# q1 cod
zero_q1_cod_ml <- test_cod_q1 |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(Year, Country, zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100,
         source = "ml")

zero_q1_cod_ale <- orio_cod_q1 |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(Year, Country, zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100,
         source = "ale")

bind_rows(zero_q1_cod_ale, zero_q1_cod_ml) |> 
  filter(zero_catch == "Y") |> 
  ggplot(aes(Year, per, color = source, linetype = source)) +
  geom_line() +
  geom_point() + 
  facet_wrap(~Country, scales = "free")

# q4 cod
zero_q4_cod_ml <- test_cod_q4 |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(Year, Country, zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100,
         source = "ml")

zero_q4_cod_ale <- orio_cod_q4 |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(Year, Country, zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100,
         source = "ale")

bind_rows(zero_q4_cod_ale, zero_q4_cod_ml) |> 
  filter(zero_catch == "Y") |> 
  ggplot(aes(Year, per, color = source, linetype = source)) +
  geom_line() +
  geom_point() + 
  facet_wrap(~Country, scales = "free")


# q1 fle
zero_q1_fle_ml <- test_fle_q1 |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(Year, Country, zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100,
         source = "ml")

zero_q1_fle_ale <- orio_fle_q1 |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(Year, Country, zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100,
         source = "ale")

bind_rows(zero_q1_fle_ale, zero_q1_fle_ml) |> 
  filter(zero_catch == "Y") |> 
  ggplot(aes(Year, per, color = source, linetype = source)) +
  geom_line() +
  geom_point() + 
  facet_wrap(~Country, scales = "free")


# q4 fle
zero_q4_fle_ml <- test_fle_q4 |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(Year, Country, zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100,
         source = "ml")

zero_q4_fle_ale <- orio_fle_q4 |> 
  mutate(zero_catch = ifelse(haul_cpue_kg == 0, "Y", "N")) |>
  group_by(Year, Country, zero_catch) |> 
  summarise(n = n()) |> 
  mutate(per = prop.table(n) * 100,
         source = "ale")

bind_rows(zero_q4_fle_ale, zero_q4_fle_ml) |> 
  filter(zero_catch == "Y") |> 
  ggplot(aes(Year, per, color = source, linetype = source)) +
  geom_line() +
  geom_point() + 
  facet_wrap(~Country, scales = "free")
```

```{r compare abundance}
test_full <- bind_rows(orio_fle_q1, orio_fle_q4, orio_cod_q1, orio_cod_q4,
                       test_cod_q1, test_cod_q4, test_fle_q1, test_fle_q4)

# Check the non-standardized data for cod
# Raw abundance cpue
test_full |> 
  filter(species == "Cod") |> 
  ggplot(., aes(factor(Year), haul_cpue_un, color = source)) + 
  geom_point(size = 1, position = position_dodge(width = 0.5)) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~Quarter) +
  NULL
  
test_full |> 
  filter(species == "Cod" & Quarter == 1) |> 
  ggplot(., aes(haul_cpue_un, fill = source)) + 
  geom_histogram(position = position_dodge()) + 
  facet_wrap(~Year, scale = "free") +
  theme(axis.text.x = element_text(angle = 90)) + 
  NULL

test_full |> 
  filter(species == "Cod" & Quarter == 4) |> 
  ggplot(., aes(haul_cpue_un, fill = source)) + 
  geom_histogram(position = position_dodge()) + 
  facet_wrap(~Year, scale = "free") +
  theme(axis.text.x = element_text(angle = 90)) + 
  NULL

# Mean abundance cpue
test_full |> 
  filter(species == "Cod") |> 
  group_by(Year, Quarter, source) |> 
  summarise(mean_cpue = mean(haul_cpue_un),
            sd_cpue = sd(haul_cpue_un)) |> 
  ggplot(., aes(Year, mean_cpue, linetype = source, color = source)) + 
  geom_line(size = 1.1) +
  geom_point(size = 3) + 
  facet_wrap(~Quarter, ncol = 1) +
  NULL

# Mean abundance cpue by country
test_full |> 
  filter(species == "Cod") |> 
  group_by(Year, source, Quarter, Country) |> 
  summarise(mean_cpue = mean(haul_cpue_un),
            sd_cpue = sd(haul_cpue_un)) |> 
  ggplot(., aes(Year, mean_cpue, linetype = source, color = source)) + 
  geom_line(size = 1.1) +
  facet_grid(Quarter ~ Country) +
  NULL

# Mean abundance cpue for non-zero cathes
test_full |> 
  filter(species == "Cod") |> 
  filter(haul_cpue_un > 0) |> 
  group_by(Year, Quarter, source) |> 
  summarise(mean_cpue = mean(haul_cpue_un),
            sd_cpue = sd(haul_cpue_un)) |> 
  ggplot(., aes(Year, mean_cpue, linetype = source, color = source)) + 
  geom_line(size = 1.1) +
  geom_point(size = 3) + 
  facet_wrap(~ Quarter, ncol = 1) +
  NULL

# Now plot corrected biomass cpue
test_full |> 
  filter(species == "Cod") |> 
  ggplot(., aes(factor(Year), haul_cpue_kg, color = source)) + 
  geom_point(size = 1, position = position_dodge(width = 0.5)) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~ Quarter, ncol = 1) +
  NULL

# Mean corrected biomass cpue
test_full |> 
  filter(species == "Cod") |> 
  group_by(Year, Quarter, source) |> 
  summarise(mean_cpue = mean(haul_cpue_kg),
            sd_cpue = sd(haul_cpue_kg)) |> 
  ggplot(., aes(Year, mean_cpue, linetype = source, color = source)) + 
  geom_line(size = 1.1) +
  geom_point(size = 3) + 
  facet_wrap(~ Quarter, ncol = 1) +
  NULL

# Now check in on flounder
# Raw abundance cpue
test_full |> 
  filter(species == "Fle") |> 
  ggplot(., aes(factor(Year), haul_cpue_un, color = source)) + 
  geom_point(size = 1, position = position_dodge(width = 0.5)) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~ Quarter, ncol = 1) +
  NULL
  
test_full |> 
  filter(species == "Fle" & Quarter == 1) |> 
  ggplot(., aes(haul_cpue_un, fill = source)) + 
  geom_histogram(position = position_dodge()) + 
  facet_wrap(~Year, scale = "free") +
  theme(axis.text.x = element_text(angle = 90)) + 
  NULL

test_full |> 
  filter(species == "Fle" & Quarter == 4) |> 
  ggplot(., aes(haul_cpue_un, fill = source)) + 
  geom_histogram(position = position_dodge()) + 
  facet_wrap(~Year, scale = "free") +
  theme(axis.text.x = element_text(angle = 90)) + 
  NULL

# Mean abundance cpue
test_full |> 
  filter(species == "Fle") |> 
  group_by(Year, Quarter, source) |> 
  summarise(mean_cpue = mean(haul_cpue_un),
            sd_cpue = sd(haul_cpue_un)) |> 
  ggplot(., aes(Year, mean_cpue, linetype = source, color = source)) + 
  geom_line(size = 1.1) +
  geom_point(size = 3) + 
  facet_wrap(~ Quarter, ncol = 1) +
  NULL

# Ok, here we can see that Ale predicts an increase earlier, which we also saw 
# on the plot of raw catches as points
# Mean abundance cpue by country to see how this can arise
test_full |> 
  filter(species == "Fle") |> 
  group_by(Year, Quarter, source, Country) |> 
  summarise(mean_cpue = mean(haul_cpue_un),
            sd_cpue = sd(haul_cpue_un)) |> 
  ggplot(., aes(Year, mean_cpue, linetype = source, color = source)) + 
  geom_line(size = 1.1) +
  facet_grid(Quarter ~ Country) +
  NULL

# Ok, after checking, the reason I don't have the older LAT data is because I don't
# have RS or RSA-values for those hauls, even though the catch data are in datras...
# Should probably check with Ale how he corrected those!

# Now plot corrected biomass cpue
test_full |> 
  filter(species == "Fle") |> 
  ggplot(., aes(factor(Year), haul_cpue_kg, color = source)) + 
  geom_point(size = 1, position = position_dodge(width = 0.5)) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~ Quarter, ncol = 1) +
  NULL

# Mean corrected biomass cpue
test_full |> 
  filter(species == "Fle") |> 
  group_by(Year, Quarter, source) |> 
  summarise(mean_cpue = mean(haul_cpue_kg),
            sd_cpue = sd(haul_cpue_kg)) |> 
  ggplot(., aes(Year, mean_cpue, linetype = source, color = source)) + 
  geom_line(size = 1.1) +
  geom_point(size = 3) + 
  facet_wrap(~ Quarter, ncol = 1) +
  NULL

# Then check length-weight relationships again
```
